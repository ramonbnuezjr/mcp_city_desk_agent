#!/usr/bin/env python3
"""
Comprehensive Test Script for MCP City Desk Agent
Uses synthetic data and prompt library for full system validation
"""

import asyncio
import json
from pathlib import Path
from src.mcp_server.testing.synthetic_data import synthetic_data_generator
from src.mcp_server.testing.prompt_library import prompt_library
from src.mcp_server.rag.chromadb_manager import ChromaDBManager
from src.mcp_server.rag.document_processor import DocumentProcessor
from src.mcp_server.rag.query_engine import RAGQueryEngine
from src.mcp_server.llm.llm_client import llm_client
from datetime import datetime

async def test_synthetic_data_generation():
    """Test synthetic data generation"""
    print("Testing Synthetic Data Generation...")
    print("=" * 50)
    
    try:
        # Generate all test data
        result = synthetic_data_generator.generate_all_test_data()
        
        if result["success"]:
            print(f"‚úÖ Generated {len(result['files_generated'])} test data files")
            print(f"   Data types: {', '.join(result['data_types'])}")
            
            # Get summary
            summary = synthetic_data_generator.get_test_data_summary()
            print(f"   Total files: {summary['total_files']}")
            print(f"   Directory: {summary['data_directory']}")
            
            return True
        else:
            print(f"‚ùå Failed to generate test data: {result.get('error')}")
            return False
            
    except Exception as e:
        print(f"‚ùå Synthetic data generation test failed: {e}")
        return False

async def test_prompt_library():
    """Test prompt library functionality"""
    print("\n\nTesting Prompt Library...")
    print("=" * 50)
    
    try:
        # Get prompt statistics
        stats = prompt_library.get_prompt_statistics()
        print(f"‚úÖ Prompt library loaded successfully")
        print(f"   Total prompts: {stats['total_prompts']}")
        print(f"   Categories: {', '.join(stats['by_category'].keys())}")
        
        # Test different prompt retrieval methods
        print("\n   Testing prompt retrieval methods:")
        
        # Get prompts by category
        data_retrieval_prompts = prompt_library.get_prompts_by_category("data_retrieval")
        print(f"     Data retrieval prompts: {len(data_retrieval_prompts)}")
        
        # Get prompts by complexity
        simple_prompts = prompt_library.get_prompts_by_complexity("simple")
        print(f"     Simple prompts: {len(simple_prompts)}")
        
        # Get prompts by difficulty
        easy_prompts = prompt_library.get_prompts_by_difficulty(1, 2)
        print(f"     Easy prompts (difficulty 1-2): {len(easy_prompts)}")
        
        # Get random prompt
        random_prompt = prompt_library.get_random_prompt()
        print(f"     Random prompt: {random_prompt['description']}")
        
        # Export prompts for testing
        export_success = prompt_library.export_prompts_for_testing()
        if export_success:
            print(f"     Exported prompts to: {prompt_library.prompts_dir}/test_prompts.json")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Prompt library test failed: {e}")
        return False

async def test_rag_with_synthetic_data():
    """Test RAG system with synthetic data"""
    print("\n\nTesting RAG System with Synthetic Data...")
    print("=" * 50)
    
    try:
        # Initialize RAG components
        chroma_manager = ChromaDBManager("./test_synthetic_chroma_db")
        doc_processor = DocumentProcessor()
        rag_engine = RAGQueryEngine(chroma_manager, doc_processor)
        
        # Check if synthetic data exists
        synthetic_data_dir = Path("./synthetic_data")
        if not synthetic_data_dir.exists():
            print("   ‚ö†Ô∏è  No synthetic data found. Generating now...")
            await test_synthetic_data_generation()
        
        # Process synthetic documents
        print("   Processing synthetic documents...")
        
        # Process building codes
        building_codes_file = synthetic_data_dir / "building_codes.txt"
        if building_codes_file.exists():
            with open(building_codes_file, 'r') as f:
                building_codes_content = f.read()
            
            # Process and add to ChromaDB
            documents = doc_processor._chunk_text(building_codes_content)
            success = chroma_manager.add_documents([
                {
                    "id": f"synth_building_{i}",
                    "text": chunk,
                    "metadata": {
                        "source": "synthetic_building_codes.txt",
                        "document_type": "building_codes",
                        "chunk_index": i
                    }
                }
                for i, chunk in enumerate(documents)
            ])
            
            if success:
                print(f"     ‚úÖ Added {len(documents)} building code chunks to ChromaDB")
            else:
                print(f"     ‚ùå Failed to add building code chunks")
        
        # Process zoning regulations
        zoning_file = synthetic_data_dir / "zoning_regulations.txt"
        if zoning_file.exists():
            with open(zoning_file, 'r') as f:
                zoning_content = f.read()
            
            documents = doc_processor._chunk_text(zoning_content)
            success = chroma_manager.add_documents([
                {
                    "id": f"synth_zoning_{i}",
                    "text": chunk,
                    "metadata": {
                        "source": "synthetic_zoning_regulations.txt",
                        "document_type": "zoning_regulations",
                        "chunk_index": i
                    }
                }
                for i, chunk in enumerate(documents)
            ])
            
            if success:
                print(f"     ‚úÖ Added {len(documents)} zoning regulation chunks to ChromaDB")
            else:
                print(f"     ‚ùå Failed to add zoning regulation chunks")
        
        # Test RAG queries with synthetic data
        print("\n   Testing RAG queries with synthetic data...")
        
        test_queries = [
            "building permit requirements",
            "zoning regulations for residential use",
            "environmental compliance standards",
            "permit processing time"
        ]
        
        for query in test_queries:
            print(f"\n     Query: '{query}'")
            try:
                result = await rag_engine.query_documents(query, n_results=3)
                if result["success"]:
                    print(f"       Results: {result['results_count']} documents found")
                    for i, doc in enumerate(result["results"][:2]):
                        print(f"         {i+1}. Score: {doc['relevance_score']:.3f}")
                        if "document_text" in doc:
                            text_preview = doc["document_text"][:80] + "..."
                            print(f"            Preview: {text_preview}")
                else:
                    print(f"       Error: {result.get('error', 'Unknown error')}")
            except Exception as e:
                print(f"       Query failed: {e}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå RAG with synthetic data test failed: {e}")
        return False

async def test_llm_with_prompts():
    """Test LLM system with prompt library"""
    print("\n\nTesting LLM System with Prompt Library...")
    print("=" * 50)
    
    try:
        # Check available LLM providers
        available_providers = llm_client.get_available_providers()
        print(f"   Available LLM providers: {', '.join(available_providers)}")
        
        if not available_providers:
            print("   ‚ö†Ô∏è  No LLM providers configured. Skipping LLM tests.")
            return True
        
        # Get some test prompts
        simple_prompts = prompt_library.get_prompts_by_complexity("simple")
        if not simple_prompts:
            print("   ‚ö†Ô∏è  No simple prompts available. Skipping LLM tests.")
            return True
        
        # Test with a simple prompt
        test_prompt = simple_prompts[0]
        print(f"   Testing with prompt: {test_prompt['description']}")
        print(f"   Prompt text: {test_prompt['prompt']}")
        
        # Test LLM invocation (if providers are available)
        try:
            # Use first available provider
            provider = available_providers[0]
            print(f"   Testing with provider: {provider}")
            
            # Simple test without context
            result = await llm_client.invoke(provider, test_prompt['prompt'])
            
            if "error" not in result:
                print(f"     ‚úÖ LLM response received")
                print(f"     Provider: {result['provider']}")
                print(f"     Model: {result['model']}")
                print(f"     Tokens used: {result['usage']['total_tokens']}")
                print(f"     Response preview: {result['response'][:100]}...")
            else:
                print(f"     ‚ùå LLM invocation failed: {result.get('error')}")
                
        except Exception as e:
            print(f"     ‚ö†Ô∏è  LLM test skipped (likely no API keys): {e}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå LLM with prompts test failed: {e}")
        return False

async def test_system_integration():
    """Test full system integration"""
    print("\n\nTesting System Integration...")
    print("=" * 50)
    
    try:
        # Test data flow: Synthetic Data ‚Üí RAG ‚Üí LLM
        print("   Testing end-to-end data flow...")
        
        # 1. Generate synthetic data
        print("     1. Generating synthetic data...")
        data_result = await test_synthetic_data_generation()
        
        # 2. Test RAG system
        print("     2. Testing RAG system...")
        rag_result = await test_rag_with_synthetic_data()
        
        # 3. Test LLM integration
        print("     3. Testing LLM integration...")
        llm_result = await test_llm_with_prompts()
        
        # Overall assessment
        if data_result and rag_result and llm_result:
            print("\n   ‚úÖ System integration test completed successfully!")
            return True
        else:
            print("\n   ‚ö†Ô∏è  System integration test completed with some issues")
            return False
            
    except Exception as e:
        print(f"‚ùå System integration test failed: {e}")
        return False

async def generate_test_report():
    """Generate comprehensive test report"""
    print("\n\nGenerating Test Report...")
    print("=" * 50)
    
    try:
        report = {
            "test_timestamp": datetime.utcnow().isoformat(),
            "system_components": {
                "synthetic_data": "Available",
                "prompt_library": "Available", 
                "rag_system": "Available",
                "llm_integration": "Available",
                "rate_limiting": "Available"
            },
            "test_data_summary": synthetic_data_generator.get_test_data_summary(),
            "prompt_library_stats": prompt_library.get_prompt_statistics(),
            "recommendations": [
                "Use synthetic data for consistent testing",
                "Leverage prompt library for systematic validation",
                "Test RAG system with various query complexities",
                "Validate LLM responses against expected formats",
                "Monitor rate limiting during high-volume testing"
            ]
        }
        
        # Save report
        report_file = Path("./test_report.json")
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"   ‚úÖ Test report generated: {report_file}")
        print(f"   Total prompts available: {report['prompt_library_stats']['total_prompts']}")
        print(f"   Test data files: {report['test_data_summary']['total_files']}")
        
        return report
        
    except Exception as e:
        print(f"‚ùå Failed to generate test report: {e}")
        return None

async def main():
    """Run comprehensive system tests"""
    print("MCP City Desk Agent - Comprehensive System Test Suite")
    print("=" * 70)
    
    # Run all tests
    test_results = {
        "synthetic_data": await test_synthetic_data_generation(),
        "prompt_library": await test_prompt_library(),
        "rag_with_synthetic_data": await test_rag_with_synthetic_data(),
        "llm_with_prompts": await test_llm_with_prompts(),
        "system_integration": await test_system_integration()
    }
    
    # Generate test report
    report = await generate_test_report()
    
    # Summary
    print("\n" + "=" * 70)
    print("COMPREHENSIVE TEST SUITE COMPLETED")
    print("=" * 70)
    
    print("\nTest Results Summary:")
    for test_name, result in test_results.items():
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"   {test_name.replace('_', ' ').title()}: {status}")
    
    passed_tests = sum(test_results.values())
    total_tests = len(test_results)
    
    print(f"\nOverall Result: {passed_tests}/{total_tests} tests passed")
    
    if passed_tests == total_tests:
        print("üéâ All tests passed! System is ready for MVP validation.")
    else:
        print("‚ö†Ô∏è  Some tests failed. Review issues before proceeding.")
    
    print("\nNext Steps:")
    print("1. Review test report for detailed results")
    print("2. Use synthetic data for consistent testing")
    print("3. Leverage prompt library for systematic validation")
    print("4. Proceed with Web Dashboard development")

if __name__ == "__main__":
    asyncio.run(main())
